##### CAP理论

数据一致性(consistency)：如果系统对一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，那么所有读操作都不能读到这个数据，对调用者而言数据具有强一致性(strong consistency)
服务可用性(availability)：所有读写请求在一定时间内得到响应，可终止、不会一直等待
分区容错性(partition-tolerance)：在网络分区的情况下，被分隔的节点仍能正常对外服务

如果选择了 CA 而放弃了 P，那么当发生分区现象时，为了保证 C，系统需要禁止写入，当有写入请求时，系统返回 error（例如，当前系统不允许写入），这又和 A 冲突了，因为 A 要求返回 no error 和no timeout。因此，分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构。

如果 CAP 三者可同时满足，因为允许分区容错，写操作可能在节点 1 上成功，在节点 2 上失败，这时候对于 Client 1 (读取节点1)和 Client 2(读取节点2)，就会读取到不一致的值，出现不一致的情况。如果要保持一致性，写操作必须同时失败， 也就是降低系统的可用性。

CAP证明、CP（分布式数据库、zk）、AP（抢票、购物、redis集群）

合理的分库分表策略：

例如一个商家的所有商品分布在多个库，多张表里，查询时则只能建立多个数据库连接，查完合并到一起，效率太低，性能消耗太大。

商家端商品表：商家id作为分表策略，同一个商家id的商品在一张表里；

用户端订单表：userId后四位作为分表策略，基于用户id可以在一张表里查到全部订单；订单id存时加上userId后四位，基于订单id后四位也能找到需要去哪张表里查全部订单。

运营管理端：查全量数据，非实时查询（数仓，等几十秒），实时（es）

旧系统上亿单表数据如何做分库分表：写一个数据迁移系统，主要职责是批量查询数据，并入到分库分表新系统里+cannal监听binlog写入增量更新的数据+增量更新和全量更新冲突的部分需要处理，可以放入mq里一段时间后再replay一下

灰度发布：新老系统都存在，一部分流量打到新系统中，检查日志是否有问题，等一段时间后再把新系统增加到满，把老系统流量全部关闭。

数据库迁移无论怎么做都会可能存在问题和bug，是否存在永不迁移的数据架构：

TiDB分布式数据库可以保证不需要分库分表

## 深入阅读：

- 
- 
- 
- 
- 
